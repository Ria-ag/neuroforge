# train_snn_surrogate.py
import math
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Function

class SurrogateSpike(Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        out = (input > 0.0).float()
        return out

    @staticmethod
    def backward(ctx, grad_output):
        (input,) = ctx.saved_tensors
        beta = 6.0  # slope of surrogate derivative (tune if needed)
        s = torch.sigmoid(beta * input)
        surrogate_grad = beta * s * (1 - s)
        return grad_output * surrogate_grad

spike_fn = SurrogateSpike.apply

class LIFCell(nn.Module):
    def __init__(self, tau=10.0, dt=1.0, v_rest=-65.0, v_reset=-65.0, v_th=-50.0):
        super().__init__()
        self.tau = tau
        self.dt = dt
        self.register_buffer('v_rest', torch.tensor(0.0))
        self.register_buffer('v_reset', torch.tensor(0.0))
        self.register_buffer('v_th', torch.tensor(1.0))

    def init_state(self, shape, device=None):
        return torch.zeros(shape, device=device)

    def forward_step(self, v, I):
        dv = (-(v - self.v_rest) + I) / self.tau
        v = v + dv * self.dt
        s = spike_fn(v - self.v_th)  # 0/1
        v = torch.where(s > 0, self.v_reset, v)
        return s, v

class SimpleTrainableSNN(nn.Module):
    def __init__(self, n_in, n_hidden, n_out, dt=1.0, tau=10.0, device='cpu'):
        super().__init__()
        self.n_in = n_in
        self.n_hidden = n_hidden
        self.n_out = n_out
        self.dt = dt
        self.device = device

        self.W1 = nn.Parameter(torch.randn(n_hidden, n_in) * 0.5)  # input -> hidden
        self.W2 = nn.Parameter(torch.randn(n_out, n_hidden) * 0.5)  # hidden -> output

        self.h_cell = LIFCell(tau=tau, dt=dt).to(device)
        self.o_cell = LIFCell(tau=tau, dt=dt).to(device)

    def forward(self, inputs):
        T, B, _ = inputs.shape
        device = inputs.device

        v_h = self.h_cell.init_state((B, self.n_hidden), device=device)
        v_o = self.o_cell.init_state((B, self.n_out), device=device)

        hidden_spikes = []
        output_spikes = []

        for t in range(T):
            inp_t = inputs[t]                    
            I_h = torch.matmul(inp_t, self.W1.t())
            s_h, v_h = self.h_cell.forward_step(v_h, I_h)
            hidden_spikes.append(s_h)

            I_o = torch.matmul(s_h, self.W2.t())  
            s_o, v_o = self.o_cell.forward_step(v_o, I_o)
            output_spikes.append(s_o)

        hidden_spikes = torch.stack(hidden_spikes, dim=0)  
        output_spikes = torch.stack(output_spikes, dim=0) 
        return hidden_spikes, output_spikes

def make_dataset(n_samples=1000, T=100, n_in=3, dt=1.0, device='cpu'):
    inputs = torch.zeros((T, n_samples, n_in), dtype=torch.float32, device=device)
    labels = torch.zeros(n_samples, dtype=torch.long, device=device)
    for i in range(n_samples):
        if np.random.rand() < 0.5:
            labels[i] = 0
            t0 = 10
            inputs[t0:t0+5, i, 0] = 1.5  # a brief stronger pulse (internal units)
        else:
            labels[i] = 1
            t0 = 30
            inputs[t0:t0+5, i, 1] = 1.5
        inputs[:, i, :] += 0.05 * torch.randn((T, n_in), device=device)
    return inputs, labels

def train_snn(device='cpu'):
    torch.manual_seed(1)
    np.random.seed(1)
    n_in = 3
    n_hidden = 16
    n_out = 2
    T = 100
    dt = 1.0
    lr = 1e-2
    batch_size = 64
    epochs = 80
    device = torch.device(device)
    model = SimpleTrainableSNN(n_in, n_hidden, n_out, dt=dt, tau=10.0, device=device).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()
    X, Y = make_dataset(n_samples=2000, T=T, n_in=n_in, device=device)
    perm = torch.randperm(X.shape[1])
    X = X[:, perm]
    Y = Y[perm]
    train_N = 1600
    X_train = X[:, :train_N, :]
    Y_train = Y[:train_N]
    X_val = X[:, train_N:, :]
    Y_val = Y[train_N:]
    print("Train samples:", X_train.shape[1], "Val samples:", X_val.shape[1])
    for ep in range(1, epochs + 1):
        model.train()
        perm = torch.randperm(X_train.shape[1])
        losses = []
        accs = []
        for i in range(0, X_train.shape[1], batch_size):
            idx = perm[i:i+batch_size]
            x_batch = X_train[:, idx, :].to(device)
            y_batch = Y_train[idx].to(device)      
            optimizer.zero_grad()
            _, out_spikes = model(x_batch)        
            logits = out_spikes.sum(dim=0)   
            loss = criterion(logits, y_batch)
            loss.backward()
            optimizer.step()
            with torch.no_grad():
                pred = logits.argmax(dim=1)
                acc = (pred == y_batch).float().mean().item()
            losses.append(loss.item())
            accs.append(acc)
        model.eval()
        with torch.no_grad():
            _, val_out = model(X_val)
            val_logits = val_out.sum(dim=0)
            val_pred = val_logits.argmax(dim=1)
            val_acc = (val_pred == Y_val).float().mean().item()
            val_loss = criterion(val_logits, Y_val).item()
        if ep % 5 == 0 or ep == 1:
            print(f"Epoch {ep:03d} | train_loss {np.mean(losses):.4f} train_acc {np.mean(accs):.3f} | val_loss {val_loss:.4f} val_acc {val_acc:.3f}")
    print("Training finished. Final val acc:", val_acc)
    return model, (X_val, Y_val)

def visualize_model(model, X_val, Y_val, device='cpu', n_examples=6):
    device = torch.device(device)
    model.eval()
    T, N, _ = X_val.shape
    n_show = min(n_examples, N)
    with torch.no_grad():
        hidden_spikes, output_spikes = model(X_val[:, :n_show, :].to(device))
    fig, axes = plt.subplots(2, n_show, figsize=(3*n_show, 4))
    for i in range(n_show):
        hs = hidden_spikes[:, i, :].cpu().numpy()  # [T, n_hidden]
        os = output_spikes[:, i, :].cpu().numpy()  # [T, n_out]
        ax = axes[0, i]
        for n in range(hs.shape[1]):
            times = np.where(hs[:, n] > 0)[0]
            ax.scatter(times, np.ones_like(times)*n, s=4, color='k')
        ax.set_title(f"Label {Y_val[i].item()} (hidden)")
        ax.set_ylim(-1, hs.shape[1]+1)
        ax.set_xlabel("time")
        ax2 = axes[1, i]
        for n in range(os.shape[1]):
            times = np.where(os[:, n] > 0)[0]
            ax2.scatter(times, np.ones_like(times)*n, s=30)
        ax2.set_title("output spikes")
        ax2.set_ylim(-1, os.shape[1]+1)
        ax2.set_xlabel("time")
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    device = 'cpu'
    model, val_data = train_snn(device=device)
    X_val, Y_val = val_data
    visualize_model(model, X_val, Y_val, device=device, n_examples=6)
